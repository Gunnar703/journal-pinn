{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from forcing_functions import get_function\n",
    "from finite_element_code import set_up_4_dof, integrate_rk4, get_mck\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set the random seed so that results are consistent across trials\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating The SIREN Class\n",
    "The SIREN network is described in [this paper](https://arxiv.org/abs/2006.09661)\n",
    "\n",
    "The main points from the paper that will be used here are:\n",
    "- Neural networks are functions of the form $u(x) = l_N \\circ l_{N-1} \\circ \\cdots \\circ l_1$ \n",
    "  - Each $l_i$ is a function representing a layer of the network\n",
    "  - $N$ is the number of layers in the network\n",
    "  - The layers take the form $l_i(x) = a( W_i x + b_i )$\n",
    "    - $a$ is the activation function (in this case $a(x) = \\sin (\\omega_0 x)$, where $\\omega_0$ is a hyper-parameter)\n",
    "\n",
    "SIRENs are especially good at handling differentiation of the network with respect to its input. This is because\n",
    "$$\\frac{d}{dx}\\sin(x) = \\cos(x) = \\sin\\left(\\frac{\\pi}{2}-x\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layers l_1 ... l_(N-1). l_N is just a linear layer, Wx + b\n",
    "class SineLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from\n",
    "    https://github.com/vsitzmann/siren/blob/master/explore_siren.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_features, out_features, bias=True, is_first=False, omega_0=30\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.omega_0 = omega_0\n",
    "        self.is_first = is_first\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        with torch.no_grad():\n",
    "            if self.is_first:\n",
    "                self.linear.weight.uniform_(-1 / self.in_features, 1 / self.in_features)\n",
    "            else:\n",
    "                self.linear.weight.uniform_(\n",
    "                    -np.sqrt(6 / self.in_features) / self.omega_0,\n",
    "                    np.sqrt(6 / self.in_features) / self.omega_0,\n",
    "                )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.sin(self.omega_0 * self.linear(input))\n",
    "\n",
    "\n",
    "# Define the SIREN network\n",
    "class Siren(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from\n",
    "    https://github.com/vsitzmann/siren/blob/master/explore_siren.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_features,\n",
    "        hidden_layers,\n",
    "        out_features,\n",
    "        outermost_linear=False,\n",
    "        first_omega_0=30,\n",
    "        hidden_omega_0=30.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = []\n",
    "        self.net.append(\n",
    "            SineLayer(\n",
    "                in_features, hidden_features, is_first=True, omega_0=first_omega_0\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for i in range(hidden_layers):\n",
    "            self.net.append(\n",
    "                SineLayer(\n",
    "                    hidden_features,\n",
    "                    hidden_features,\n",
    "                    is_first=False,\n",
    "                    omega_0=hidden_omega_0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if outermost_linear:\n",
    "            final_linear = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                final_linear.weight.uniform_(\n",
    "                    -np.sqrt(6 / hidden_features) / hidden_omega_0,\n",
    "                    np.sqrt(6 / hidden_features) / hidden_omega_0,\n",
    "                )\n",
    "\n",
    "            self.net.append(final_linear)\n",
    "        else:\n",
    "            self.net.append(\n",
    "                SineLayer(\n",
    "                    hidden_features,\n",
    "                    out_features,\n",
    "                    is_first=False,\n",
    "                    omega_0=hidden_omega_0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "    def forward(self, coords):\n",
    "        coords = (\n",
    "            coords.clone().detach().requires_grad_(True)\n",
    "        )  # allows to take derivative w.r.t. input\n",
    "        output = self.net(coords)\n",
    "\n",
    "        return output, coords\n",
    "\n",
    "\n",
    "class OtherActivationNetwork(nn.Module):\n",
    "    # Same as Siren class, except the activation function can be chosen\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_features,\n",
    "        hidden_layers,\n",
    "        out_features,\n",
    "        outermost_linear=False,\n",
    "        activation=nn.Tanh,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "\n",
    "        self.net = []\n",
    "        self.net.append(\n",
    "            nn.Sequential(nn.Linear(in_features, hidden_features), self.activation())\n",
    "        )\n",
    "\n",
    "        for _ in range(hidden_layers):\n",
    "            self.net.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(\n",
    "                        hidden_features,\n",
    "                        hidden_features,\n",
    "                    ),\n",
    "                    self.activation(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if outermost_linear:\n",
    "            final_linear = nn.Linear(hidden_features, out_features)\n",
    "            self.net.append(final_linear)\n",
    "        else:\n",
    "            self.net.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_features, out_features), self.activation()\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "    def forward(self, coords):\n",
    "        coords = (\n",
    "            coords.clone().detach().requires_grad_(True)\n",
    "        )  # allows to take derivative w.r.t. input\n",
    "        output = self.net(coords)\n",
    "\n",
    "        return output, coords\n",
    "\n",
    "\n",
    "def gradient(y, x, grad_outputs=None):\n",
    "    if grad_outputs is None:\n",
    "        grad_outputs = torch.ones_like(y)\n",
    "    grad = torch.autograd.grad(y, [x], grad_outputs=grad_outputs, create_graph=True)[0]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_TIME = 0\n",
    "FINAL_TIME = 2.9\n",
    "N_TIMESTEPS = 290  # Number of time-steps to simulate\n",
    "DOF_DATA = [2, 3]  # Which degrees of freedom to give data on\n",
    "N_DATAPOINTS = 20  # Number of datapoints to train on\n",
    "FUNCTION = \"sine\"  # options are \"sine\" \"gaussian\" \"chirp\"\n",
    "DATA_CUTOFF_TIME = 2  # Time after which to stop giving data\n",
    "\n",
    "t = torch.linspace(INITIAL_TIME, FINAL_TIME, N_TIMESTEPS, device=\"cuda\")\n",
    "\n",
    "x, y, E, nu, rho, a0, a1, fdim, free_indices, ndim, sens_dim = set_up_4_dof()\n",
    "ffun = get_function(\n",
    "    FUNCTION,\n",
    "    INITIAL_TIME,\n",
    "    FINAL_TIME,\n",
    "    amplitude=-1000,\n",
    "    total_dimensions=ndim,\n",
    "    force_dimension=fdim,\n",
    ")\n",
    "m, c, k = get_mck(x, y, E, nu, rho, a0, a1, free_indices, device=\"cuda\")\n",
    "displacements, velocities = integrate_rk4(m, c, k, ffun, t)\n",
    "\n",
    "cutoff_index = torch.argmin((t - DATA_CUTOFF_TIME).abs())\n",
    "data_indices_step = int(cutoff_index / N_DATAPOINTS)\n",
    "data_indices = torch.arange(0, cutoff_index, data_indices_step)\n",
    "\n",
    "i, j = torch.meshgrid(torch.as_tensor(DOF_DATA), data_indices, indexing=\"ij\")\n",
    "displacement_data = displacements[i, j]\n",
    "velocity_data = velocities[i, j]\n",
    "t_data = t[data_indices]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Forcing Function\")\n",
    "plt.xlabel(\"Time, $t$ [sec]\")\n",
    "plt.ylabel(\"Force, $f$ [N]\")\n",
    "plt.plot(t.cpu(), ffun(t)[fdim, :].cpu())\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Reference Displacement\")\n",
    "plt.xlabel(\"Time, $t$ [sec]\")\n",
    "plt.ylabel(\"Force, $f$ [N]\")\n",
    "plt.plot(t.cpu(), displacements.cpu().T, linewidth=1)\n",
    "plt.plot(\n",
    "    t_data.cpu(),\n",
    "    displacement_data.cpu().T,\n",
    "    linestyle=\"None\",\n",
    "    marker=\"x\",\n",
    "    color=\"purple\",\n",
    ")\n",
    "plt.legend([\"u1x\", \"u1y\", \"u2x\", \"u2y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dt\n",
    "dt = t[1] - t[0]\n",
    "\n",
    "\n",
    "# Define Physics Loss Function\n",
    "def physics_loss(v1_predicted, u0, v0, f0, dt):\n",
    "    a0_predicted = (v1_predicted - v0) / dt\n",
    "    residual_vector = m @ a0_predicted + c @ v0 + k @ u0 - f0\n",
    "    loss = torch.nn.functional.mse_loss(residual_vector, residual_vector * 0)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Class to store loss info\n",
    "@dataclass\n",
    "class LossHistory:\n",
    "    physics_loss: \"list[float]\"\n",
    "    data_loss: \"list[float]\"\n",
    "    total_loss: \"list[float]\"\n",
    "\n",
    "\n",
    "# Function that trains the model\n",
    "def train_model(model, optimizer, n_epochs, physics_weight, update_interval=10):\n",
    "\n",
    "    loss_history = LossHistory([], [], [])\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # accumulator variables\n",
    "        phys_loss = 0\n",
    "        data_loss = 0\n",
    "\n",
    "        u0 = torch.zeros(1, 4, device=\"cuda\")\n",
    "\n",
    "        v0 = torch.zeros(1, 4, device=\"cuda\")\n",
    "\n",
    "        for time_index in range(len(t)):\n",
    "\n",
    "            t0 = t[time_index]\n",
    "\n",
    "            f0 = ffun(t0)\n",
    "\n",
    "            f0_magnitude = f0.max()\n",
    "\n",
    "            model_input = torch.hstack(\n",
    "                (\n",
    "                    u0,\n",
    "                    v0,\n",
    "                    f0_magnitude.reshape(1, -1),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            v1_predicted, _ = model(model_input)\n",
    "\n",
    "            phys_loss = phys_loss + physics_weight * physics_loss(\n",
    "                v1_predicted=v1_predicted,\n",
    "                u0=u0.T,\n",
    "                v0=v0.T,\n",
    "                f0=f0,\n",
    "                dt=dt,\n",
    "            )\n",
    "\n",
    "            # Data loss\n",
    "\n",
    "            if time_index in data_indices:\n",
    "\n",
    "                u1_predicted = u0 + v0 * dt\n",
    "\n",
    "                # Select u1_predicted at nodes where data is available\n",
    "\n",
    "                u1_predicted_data = u1_predicted[:, DOF_DATA]\n",
    "\n",
    "                v1_predicted_data = v1_predicted[:, DOF_DATA]\n",
    "\n",
    "                displacement_error = torch.nn.functional.mse_loss(\n",
    "                    u1_predicted_data.squeeze(),\n",
    "                    displacement_data[:, time_index % data_indices_step],\n",
    "                )\n",
    "\n",
    "                velocity_error = torch.nn.functional.mse_loss(\n",
    "                    v1_predicted_data.squeeze(),\n",
    "                    velocity_data[:, time_index % data_indices_step],\n",
    "                )\n",
    "\n",
    "                data_loss = data_loss + (displacement_error + velocity_error) * (\n",
    "                    4 * len(t)\n",
    "                ) / (N_DATAPOINTS * len(DOF_DATA))\n",
    "\n",
    "            # Propagate u and v\n",
    "\n",
    "            u0 = u0 + v0 * dt\n",
    "\n",
    "            v0 = v1_predicted.detach()\n",
    "\n",
    "        # Add losses\n",
    "        loss = data_loss + phys_loss\n",
    "\n",
    "        # Store losses for graphing later\n",
    "        loss_history.data_loss.append(float(data_loss.detach().item()))\n",
    "        loss_history.physics_loss.append(float(phys_loss.detach().item()))\n",
    "        loss_history.total_loss.append(float(loss.detach().item()))\n",
    "\n",
    "        # Backpropagate loss throughtout network - get derivatives of W and b w.r.t. loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the network parameters\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if not (epoch % update_interval):\n",
    "            pbar.set_description_str(\n",
    "                f\"Epoch: {epoch} - L_p={float(phys_loss): 4g} - L_c={float(data_loss): 4g}\"\n",
    "            )\n",
    "\n",
    "    return model, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Visualization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(model, savefile, restrict_bounds_to_true_sol=False):\n",
    "    all_state_vectors = torch.zeros(len(t), 8, device=\"cuda\")\n",
    "\n",
    "    u0 = all_state_vectors[0, :4].reshape(1, -1)\n",
    "    v0 = all_state_vectors[0, 4:].reshape(1, -1)\n",
    "\n",
    "    for time_index in range(len(t)):\n",
    "        t0 = t[time_index]\n",
    "\n",
    "        f0 = ffun(t0)\n",
    "\n",
    "        f0_magnitude = f0.max()\n",
    "\n",
    "        model_input = torch.hstack(\n",
    "            (\n",
    "                u0,\n",
    "                v0,\n",
    "                f0_magnitude.reshape(1, -1),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        v1_predicted, _ = model(model_input)\n",
    "        u0 = u0 + v0 * dt\n",
    "\n",
    "        v0 = v1_predicted.detach()\n",
    "\n",
    "        all_state_vectors[time_index, :4] = u0.squeeze()\n",
    "        all_state_vectors[time_index, 4:] = v0.squeeze()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "    for i in range(axes.shape[0]):\n",
    "        for j in range(axes.shape[1]):\n",
    "            ax = axes[i, j]\n",
    "            # i-th Node\n",
    "            # j-th DOF\n",
    "\n",
    "            # EXAMPLE: node 1, y-direction -> i = 0, j = 1 -> node_idx = 2*0 + 1 = 1 (second element in the array)\n",
    "            node_idx = 2 * i + j\n",
    "\n",
    "            # Get model prediction for time-history at this node\n",
    "            model_prediction_u = all_state_vectors[:, node_idx].squeeze()\n",
    "\n",
    "            # '4 + ...' is necessary because state vector looks like [u1x u1y u2x u2y v1x v1y v2x v2y]\n",
    "            #                                                                         ^- index 4\n",
    "            # model_prediction_v = all_state_vectors[:, 4 + node_idx].squeeze()\n",
    "\n",
    "            # Get ground-truth (reference solution)\n",
    "            true_u = displacements[node_idx, :].squeeze()\n",
    "            # true_v = velocities[node_idx, :].squeeze()\n",
    "\n",
    "            # Get given data\n",
    "            ct = 0\n",
    "            if node_idx in DOF_DATA:\n",
    "                t_data = t[data_indices]\n",
    "                u_data = displacement_data[ct].squeeze()\n",
    "                # v_data = velocity_data[ct].squeeze()\n",
    "                ct += 1\n",
    "\n",
    "            # Make plots\n",
    "            ax.plot(t.cpu(), true_u.cpu(), color=\"red\", label=\"Reference\")\n",
    "            ax.plot(\n",
    "                t.cpu(),\n",
    "                model_prediction_u.cpu(),\n",
    "                color=\"#808080\",\n",
    "                linestyle=\"dashed\",\n",
    "                label=\"Prediction\",\n",
    "            )\n",
    "\n",
    "            if node_idx in DOF_DATA:\n",
    "                ax.plot(\n",
    "                    t_data.cpu(),\n",
    "                    u_data.cpu(),\n",
    "                    color=\"black\",\n",
    "                    marker=\"x\",\n",
    "                    linestyle=\"none\",\n",
    "                    label=\"Data\",\n",
    "                )\n",
    "\n",
    "            if i == 0 and j == 0:\n",
    "                ax.legend()\n",
    "\n",
    "            ax.set_title(f\"$u_{{ {i + 1} { 'x' if j == 0 else 'y' } }}$\")\n",
    "\n",
    "            if restrict_bounds_to_true_sol:\n",
    "                ax.set_ylim(displacements.min().cpu(), displacements.max().cpu())\n",
    "\n",
    "    fig.suptitle(\"Prediction vs. Ground Truth\")\n",
    "    fig.supxlabel(\"Time, $t$ [sec]\")\n",
    "    fig.supylabel(\"Displacement, $u$ [m]\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(savefile, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_loss(loss_history, savefile):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(loss_history.total_loss, color=\"black\", label=\"Total\")\n",
    "    plt.plot(\n",
    "        loss_history.data_loss, color=\"blue\", linestyle=\"dashed\", label=\"Data\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        loss_history.physics_loss,\n",
    "        color=\"red\",\n",
    "        linestyle=\"dashed\",\n",
    "        label=\"Physics (Weighted)\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss History\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(savefile, bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "N_EPOCHS = 10_000\n",
    "\n",
    "model = Siren(\n",
    "    in_features=4 + 4 + 1,  # 4 displacement, 4 velocity, 1 forcing\n",
    "    hidden_features=32,\n",
    "    hidden_layers=3,\n",
    "    out_features=4,  # 4 velocities\n",
    "    outermost_linear=True,\n",
    ").to(device=\"cuda\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model, loss_history = train_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    physics_weight=1e-16,\n",
    ")\n",
    "\n",
    "visualize(\n",
    "    model=model,\n",
    "    savefile=\"test.png\",\n",
    "    restrict_bounds_to_true_sol=True,\n",
    ")\n",
    "\n",
    "plot_loss(\n",
    "    loss_history=loss_history,\n",
    "    savefile=\"testloss.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
